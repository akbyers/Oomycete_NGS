---
title: "oomycete_dada2"
output: html_document
date: "2025-09-19"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

##More detailed description of tutorial provided by https://benjjneb.github.io/dada2/ITS_workflow.html
```{r}
#if (!requireNamespace("BiocManager", quietly = TRUE))
 #   install.packages("BiocManager")
BiocManager::install("dada2")
```
```{r load libraries}
library(dada2)
packageVersion("dada2")
library(ShortRead)
packageVersion("ShortRead")
library(Biostrings)
packageVersion("Biostrings")
```
```{r define path variable to directory containing data}
path <- "C:/Users/byersa/OneDrive - Lincoln University/Documents/Rutherford/Oomycete sequencing/AGRF_NXGSQCAGRF25070329-1_M743W/AGRF_NXGSQCAGRF25070329-1_M743W" # CHANGE ME to the directory containing the fastq files after unzipping.
list.files(path)
```
```{r read in names of fastq files}
fnFs <- sort(list.files(path, pattern = "_R1.fastq.gz", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "_R2.fastq.gz", full.names = TRUE))
```
#Identify primers
```{r}
FWD <- "TCGTCGGCAGCGTCAGATGTGTATAAGAGACAG" #OOMUP18Sc
REV <- "GTCTCGTGGGCTCGGAGATGTGTATAAGAGACAG" #ITS2P

allOrients <- function(primer) {
    # Create all orientations of the input sequence
    require(Biostrings)
    dna <- DNAString(primer)  # The Biostrings works w/ DNAString objects rather than character vectors
    orients <- c(Forward = dna, Complement = Biostrings::complement(dna), Reverse = Biostrings::reverse(dna),
        RevComp = Biostrings::reverseComplement(dna))
    return(sapply(orients, toString))  # Convert back to character vector
}
FWD.orients <- allOrients(FWD)
REV.orients <- allOrients(REV)
FWD.orients
```
```{r pre-filter to remove Ns}
fnFs.filtN <- file.path(path, "filtN", basename(fnFs)) # Put N-filtered files in filtN/ subdirectory
fnRs.filtN <- file.path(path, "filtN", basename(fnRs))
filterAndTrim(fnFs, fnFs.filtN, fnRs, fnRs.filtN, maxN = 0, multithread = TRUE)
```
```{r}
primerHits <- function(primer, fn) {
    # Counts number of reads in which the primer is found
    nhits <- vcountPattern(primer, sread(readFastq(fn)), fixed = FALSE)
    return(sum(nhits > 0))
}
rbind(FWD.ForwardReads = sapply(FWD.orients, primerHits, fn = fnFs.filtN[[1]]), FWD.ReverseReads = sapply(FWD.orients,
    primerHits, fn = fnRs.filtN[[1]]), REV.ForwardReads = sapply(REV.orients, primerHits,
    fn = fnFs.filtN[[1]]), REV.ReverseReads = sapply(REV.orients, primerHits, fn = fnRs.filtN[[1]]))

##primers are present in RevComp - they need to be removed
REV <- reverseComplement(DNAString(REV))
REV <- as.character(REV)
```
#Primers were removed in Linux using cutadapt - cutadapt_script.sh for methods
```{r set file path for trimmed reads}
path.cut <- "C:/Users/byersa/OneDrive - Lincoln University/Documents/Rutherford/Oomycete sequencing/AGRF_NXGSQCAGRF25070329-1_M743W/AGRF_NXGSQCAGRF25070329-1_M743W/trimmed_reads"  ## CHANGE ME to the directory containing the fastq files.
list.files(path.cut)
```
```{r}
# Forward and reverse fastq filenames have the format:
cutFs <- sort(list.files(path.cut, pattern = "_R1.trimmed.fastq.gz", full.names = TRUE))
cutRs <- sort(list.files(path.cut, pattern = "_R2.trimmed.fastq.gz", full.names = TRUE))

# Extract sample names, assuming filenames have format:
get.sample.name <- function(fname) strsplit(basename(fname), "-")[[1]][1]
sample.names <- unname(sapply(cutFs, get.sample.name))
head(sample.names)
```
```{r Inspect read quality profiles}
plotQualityProfile(cutFs[1:2])
plotQualityProfile(cutRs[1:2])
```
```{r Filter and trim}
filtFs <- file.path(path.cut, "filtered", basename(cutFs))
filtRs <- file.path(path.cut, "filtered", basename(cutRs))

#standard filtering paraments
out <- filterAndTrim(cutFs, filtFs, cutRs, filtRs, maxN = 0, maxEE = c(2, 2), truncQ = 2,
    minLen = 50, rm.phix = TRUE, compress = TRUE, multithread = TRUE)  # on windows, set multithread = FALSE
head(out)
```
#Learn the error rates
```{r}
errF <- learnErrors(filtFs, multithread = FALSE)
errR <- learnErrors(filtRs, multithread = FALSE)
plotErrors(errF, nominalQ = TRUE)
```
#Sample inference
```{r}
dadaFs <- dada(filtFs, err = errF, multithread = TRUE)
dadaRs <- dada(filtRs, err = errR, multithread = TRUE)
```
#Merge paired reads
```{r}
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
```
#Construct Sequence Table
```{r}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
```
#Remove chimeras
```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
table(nchar(getSequences(seqtab.nochim)))
```
#Track reads through the pipeline
```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN),
    rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace
# sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
write.csv(track, "C:/Users/byersa/OneDrive - Lincoln University/Documents/Rutherford/Oomycete sequencing/DADA2_outputs/track_sequences.csv")
```

```{r export tables}
# giving our seq headers more manageable names (ASV_1, ASV_2...)
asv_seqs <- colnames(seqtab.nochim)
asv_headers <- vector(dim(seqtab.nochim)[2], mode="character")

for (i in 1:dim(seqtab.nochim)[2]) {
    asv_headers[i] <- paste(">ASV", i, sep="_")
}

    # making and writing out a fasta of our final ASV seqs:
asv_fasta <- c(rbind(asv_headers, asv_seqs))
write(asv_fasta, "C:/Users/byersa/OneDrive - Lincoln University/Documents/Rutherford/Oomycete sequencing/DADA2_outputs/ASV_sequences.fa")

    # count table:
asv_tab <- t(seqtab.nochim)
row.names(asv_tab) <- sub(">", "", asv_headers)
write.csv(asv_tab, "C:/Users/byersa/OneDrive - Lincoln University/Documents/Rutherford/Oomycete sequencing/DADA2_outputs/ASV_counts.csv")
```

##Merging into 97% OTUs using Vsearch
##THE FOLLOWING STEPS WERE UNDERTAKEN IN LINUX NOT R - WRITTEN BELOW FOR DOCUMENTATION ONLY - DO NOT RUN THIS CODE IN R ENVIRONMENT
```{r MERGE INTO 97% OTUS IN LINUX}
#cd /home/byersa/00_nesi_projects/lincoln03750/oomycete_AGRF/AGRF_NXGSQCAGRF25070329-1_M743W
#module purge
#module load VSEARCH/2.21.1-GCC-11.3.0

#Cluster ASV sequences at 97% to get OTU centroids
#vsearch --cluster_size ASV_sequences.fa \
#        --id 0.97 \
#        --centroids otus_97.fasta \
#        --relabel OTU_ \
#        --sizein --sizeout
#Map ASVs back to OTU centroids (global mapping)
#vsearch --usearch_global ASV_sequences.fa \
#        --db otus_97.fasta \
#        --id 0.97 \
#        --strand plus \
#        --uc asv2otu.uc \
#        --threads 4
```
##NOW BACK IN R ENVIRONMENT: COLLAPSE ASV TABLE INTO OTU TABLE
```{r}
suppressWarnings(library(tidyverse))
library(Matrix)
```
```{r}
# read ASV fasta headers to get ASV IDs (if needed)
# Alternatively, if seqtab colnames are ASV sequences, convert accordingly.
# Here we assume seqtab columns are ASV ids matching fasta headers (ASV1, ASV2...)

# read mapping (from vsearch uc file)
uc <- read.table("C:/Users/byersa/OneDrive - Lincoln University/Documents/Rutherford/Oomycete sequencing/DADA2_outputs/asv2otu.uc", sep="\t", quote="", stringsAsFactors=FALSE, fill=TRUE)

# Only keep rows with clusters
uc <- uc[uc$V1 %in% c("S","H"), ]

# Build ASV -> OTU map
map <- data.frame(
  ASV = ifelse(uc$V1=="S", uc$V9, uc$V9),   # ASV ID
  OTU = ifelse(uc$V1=="S", uc$V9, uc$V10)  # OTU ID (centroid)
)

# Clean OTU names: remove ";size=XX"
map$OTU <- sub(";.*", "", map$OTU)
```
```{r}
# Collapse ASVs into OTUs (ASVs = rows, samples = columns)
otu_list <- lapply(split(map$ASV, map$OTU), function(asvs) {
  colSums(asv_tab[asvs, , drop=FALSE])
})

# Combine list into data frame
otu_table <- do.call(rbind, otu_list)

# Verify
dim(otu_table)           # OTUs x samples
head(otu_table)
```
```{r}
##Clean up sample names in OTU table file
colnames(otu_table) <- sub("-Amplicon-Indexing.*", "", colnames(otu_table))
```

```{r}
#Save OTU table to file
write.table(otu_table, "C:/Users/byersa/OneDrive - Lincoln University/Documents/Rutherford/Oomycete sequencing/DADA2_outputs/otu_table_97.tsv", sep="\t", quote=FALSE, col.names=NA)
```
#Sanity check: ensure OTUs in abundance table match OTUs in fasta file
```{r}
otu_ids_table <- rownames(otu_table) 
fasta <- readDNAStringSet("C:/Users/byersa/OneDrive - Lincoln University/Documents/Rutherford/Oomycete sequencing/DADA2_outputs/otus_97.fasta")
fasta_ids <- names(fasta)

# Clean IDs in FASTA too
fasta_ids_clean <- sub(";.*", "", fasta_ids)
```
```{r Compare IDs}
# How many match?
sum(otu_ids_table %in% fasta_ids_clean)

# Which are missing in the table but present in fasta?
setdiff(fasta_ids_clean, otu_ids_table)

# Which are in the table but not in fasta?
setdiff(otu_ids_table, fasta_ids_clean)

length(otu_ids_table) == length(fasta_ids_clean)
all(sort(otu_ids_table) == sort(fasta_ids_clean))
```
##Clean up OTU names in OTU fasta file
```{r}
# Clean names
names(fasta) <- sub(";size=.*", "", names(fasta))

# Save cleaned FASTA
writeXStringSet(fasta, "C:/Users/byersa/OneDrive - Lincoln University/Documents/Rutherford/Oomycete sequencing/DADA2_outputs/otus_97_clean.fasta") ##Use this for subsequent analysis
```

##Taxonomy was assigned against the NCBI nt database using blastn in Linux. Non-oomycetes and non-ITS1 gene regions were removed from the fasta file. See blastn_OTU.sh for more information
#Filter OTUs from OTU table that are no longer present in the fasta file and taxonomy file
```{r}
otu_table <- read.table("C:/Users/byersa/OneDrive - Lincoln University/Documents/Rutherford/Oomycete sequencing/DADA2_outputs/otu_table_97.tsv", header=TRUE, row.names=1, sep="\t", check.names=FALSE)
# Read list of Oomycete OTUs
oomycete_ids <- read.table("C:/Users/byersa/OneDrive - Lincoln University/Documents/Rutherford/Oomycete sequencing/DADA2_outputs/oomycete_ids.txt", stringsAsFactors=FALSE)[,1]
# Keep only Oomycete OTUs
otu_table_oomycetes <- otu_table[rownames(otu_table) %in% oomycete_ids, ]
# Save
write.table(otu_table_oomycetes, "C:/Users/byersa/OneDrive - Lincoln University/Documents/Rutherford/Oomycete sequencing/DADA2_outputs/otu_table_97_oomycetes.tsv", sep="\t", quote=FALSE, col.names=NA)
```

##MAFFT alignment and IQ-TREE2 placement for taxonomic identification
```{r}
library(tidyverse)
#library(devtools)
#install_github("ropensci/rentrez")
library(rentrez)
library(dplyr)
```
```{r}
blast <- read.delim("C:/Users/byersa/OneDrive - Lincoln University/Documents/Rutherford/Oomycete sequencing/DADA2_outputs/blast_oomycetes_ITS1_with_header.tsv", stringsAsFactors = FALSE, quote = "", check.names = FALSE)
blast <- blast[,-19] #remove duplicate column
blast$qseqid <- as.factor(blast$qseqid)
```
```{r}
# 1) keep hits with pident >= 80 and qcovs >=80
filtered <- blast %>%
  filter(pident >= 80, qcovs >= 80)
filtered$qseqid <- as.factor(filtered$qseqid)
```
```{r}
# 2) for each qseqid, select top N by bitscore, then add one representative per distinct genus seen
topN <- 30
selected <- filtered %>%
  group_by(qseqid) %>%
  arrange(desc(bitscore), desc(pident)) %>%
  slice_head(n = topN) %>%
  ungroup()
selected$qseqid <- as.factor(selected$qseqid)
levels(selected$qseqid)

# Optional: reduce redundancy by taxon — if you have sscinames or taxid column, use that.
# Write selected sseqids (accessions) for fetching
selected %>% distinct(sseqid) %>% write_tsv("selected_accessions.txt", col_names = FALSE)

# Assume 'selected' is your filtered BLAST table
selected <- selected %>%
  mutate(
    acc = str_extract(sseqid, "[A-Z0-9]+\\.[0-9]+")  # grabs accession.version
  )
# Remove any NA rows (failed to extract)
selected <- selected %>% filter(!is.na(acc))

# Unique accessions for fetching
accs <- unique(selected$acc)
```
```{r}
# ---- 5. Fetch sequences in batches using rentrez ----
batch_size <- 100
batches <- split(accs, ceiling(seq_along(accs) / batch_size))

seqs <- lapply(batches, function(ids) {
  entrez_fetch(db = "nuccore", id = ids, rettype = "fasta")
})

# Collapse all batches into one FASTA
all_fasta <- paste(seqs, collapse = "\n")

# ---- 6. Write output files ----
writeLines(all_fasta, "selected_sequences.fasta")        # FASTA of references
write_tsv(selected, "filtered_hits.tsv")                 # Table with metadata
write_tsv(as_tibble(accs), "selected_accessions.txt")    # Raw list of IDs
```
##Perform query and reference alignment in Linux environment using identification_analysis.txt script
##Sequences were aligned using MAFFT and tree built using IQ-TREE2
```{r}
library(ape)
tree_td <- read.tree("C:/Users/byersa/OneDrive - Lincoln University/Documents/Rutherford/Oomycete sequencing/MAFFT_IQTREE2/comb_ref_query_seqs.trim.aln.fasta.treefile")
```
```{r Separate query OTUs from references}
query_tips <- grep("OTU_", tree$tip.label, value = TRUE)
ref_tips   <- setdiff(tree$tip.label, query_tips)
```
```{r Find the closest reference for each OTU}
install.packages("phytools")
library(phytools)
```
```{r}
# Compute distance matrix between all tips
dist_matrix <- cophenetic(tree)

# Extract only query vs reference distances
dist_sub <- dist_matrix[query_tips, ref_tips]

# Find the closest reference for each query OTU
closest_ref <- apply(dist_sub, 1, function(x) ref_tips[which.min(x)])
closest_ref
```
```{r Extract genus from reference labels}
genus <- sapply(strsplit(closest_ref, "\\|"), `[`, 2)
genus
```
```{r}
results <- data.frame(
  OTU = query_tips,
  Closest_Ref = closest_ref,
  Genus = genus
)
head(results)
write.csv(results, "iqTree_placement_results.csv")
```





















